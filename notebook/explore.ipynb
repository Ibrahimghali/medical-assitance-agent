{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beed4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core import Document\n",
    "import pprint\n",
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c684ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= SimpleDirectoryReader(input_dir= '../data', filename_as_id= False).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8d7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b833a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='0dcc0b80-1a85-47af-8fc5-db7abeb0a9b3', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[ \\nE T L \\np r o c e s s e s \\nu s i n g \\nP y S p a r k \\n] \\n# \\nQ u i c k \\nS u m m a r y \\n 1.EnvironmentSetupandSparkSessionCreation\\n●\\nInstall\\nPySpark\\n: pipinstallpyspark●\\nStart\\na\\nSparkSession\\n: frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName(\\'ETLProcess\\').getOrCreate()\\n2.DataExtraction\\n●\\nRead\\nData\\nfrom\\nCSV\\n: df= spark.read.csv(\\'path/to/csv\\',inferSchema=True,header=True)●\\nRead\\nData\\nfrom\\nJSON\\n: df= spark.read.json(\\'path/to/json\\')●\\nRead\\nData\\nfrom\\nParquet\\n: df= spark.read.parquet(\\'path/to/parquet\\')●\\nRead\\nData\\nfrom\\na\\nDatabase\\n: df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\\n3.DataTransformation\\n●\\nSelecting\\nColumns\\n: df.select(\\'column1\\',\\'column2\\')●\\nFiltering\\nData\\n: df.filter(df[\\'column\\']> value)●\\nAdding\\nNew\\nColumns\\n: df.withColumn(\\'new_column\\',df[\\'column\\']+10)●\\nRenaming\\nColumns\\n: df.withColumnRenamed(\\'old_name\\',\\'new_name\\')●\\nGrouping\\nand\\nAggregating\\nData\\n: df.groupBy(\\'column\\').agg({\\'column2\\':\\'sum\\'})●\\nJoining\\nDataFrames\\n: df1.join(df2,df1[\\'id\\']==df2[\\'id\\'])●\\nSorting\\nData\\n: df.orderBy(df[\\'column\\'].desc())●\\nRemoving\\nDuplicates\\n: df.dropDuplicates()\\n4.HandlingMissingValues\\n●\\nDropping\\nRows\\nwith\\nMissing\\nValues\\n: df.na.drop()●\\nFilling\\nMissing\\nValues\\n: df.na.fill(value)●\\nReplacing\\nValues\\n: df.na.replace([\\'old_value\\'],[\\'new_value\\'])\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='24b4264f-dfba-4654-9c92-29fa059a2b97', embedding=None, metadata={'page_label': '2', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.DataTypeConversion\\n●\\nChanging\\nColumn\\nTypes\\n: df.withColumn(\\'column\\',df[\\'column\\'].cast(\\'new_type\\'))●\\nParsing\\nDates\\n: frompyspark.sql.functionsimportto_date;df.withColumn(\\'date\\',to_date(df[\\'date_string\\']))\\n6.AdvancedDataManipulations\\n●\\nUsing\\nSQL\\nQueries\\n: df.createOrReplaceTempView(\\'table\\');spark.sql(\\'SELECT*FROMtableWHEREcolumn> value\\')●\\nWindow\\nFunctions\\n: frompyspark.sql.windowimportWindow;frompyspark.sql.functionsimportrow_number;df.withColumn(\\'row\\',row_number().over(Window.partitionBy(\\'column\\').orderBy(\\'other_column\\')))●\\nPivot\\nTables\\n:df.groupBy(\\'column\\').pivot(\\'pivot_column\\').agg({\\'column2\\':\\'sum\\'})\\n7.DataLoading\\n●\\nWriting\\nto\\nCSV\\n: df.write.csv(\\'path/to/output\\')●\\nWriting\\nto\\nJSON\\n: df.write.json(\\'path/to/output\\')●\\nWriting\\nto\\nParquet\\n: df.write.parquet(\\'path/to/output\\')●\\nWriting\\nto\\na\\nDatabase\\n: df.write.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").save()\\n8.PerformanceTuning\\n●\\nCaching\\nData\\n: df.cache()●\\nBroadcasting\\na\\nDataFrame\\nfor\\nJoin\\nOptimization\\n: frompyspark.sql.functionsimportbroadcast;df1.join(broadcast(df2),df1[\\'id\\']==df2[\\'id\\'])●\\nRepartitioning\\nData\\n: df.repartition(10)●\\nCoalescing\\nPartitions\\n: df.coalesce(1)\\n9.DebuggingandErrorHandling\\n●\\nShowing\\nExecution\\nPlan\\n: df.explain() \\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='bc54acb6-6ec0-4d4e-9a1b-0f9ea69f7e23', embedding=None, metadata={'page_label': '3', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nCatching\\nExceptions\\nduring\\nRead\\n:\\nImplement\\ntry-except\\nblocks\\nduring\\ndata\\nreading\\noperations.\\n10.WorkingwithComplexDataTypes\\n●\\nExploding\\nArrays\\n: frompyspark.sql.functionsimportexplode;df.select(explode(df['array_column']))●\\nHandling\\nStruct\\nFields\\n: df.select('struct_column.field1','struct_column.field2')\\n11.CustomTransformationswithUDFs\\n●\\nDeﬁning\\na\\nUDF\\n: frompyspark.sql.functionsimportudf;@udf('return_type')defmy_udf(column):returntransformation●\\nApplying\\nUDF\\non\\nDataFrame\\n: df.withColumn('new_column',my_udf(df['column']))\\n12.WorkingwithLargeTextData\\n●\\nTokenizing\\nText\\nData\\n: frompyspark.ml.featureimportTokenizer;Tokenizer(inputCol='text_column',outputCol='words').transform(df)●\\nTF-IDF\\non\\nText\\nData\\n: frompyspark.ml.featureimportHashingTF,IDF;HashingTF(inputCol='words',outputCol='rawFeatures').transform(df)\\n13.MachineLearningIntegration\\n●\\nUsing\\nMLlib\\nfor\\nPredictive\\nModeling\\n:\\nBuilding\\nand\\ntraining\\nmachine\\nlearning\\nmodels\\nusing\\nPySpark's\\nMLlib.●\\nModel\\nEvaluation\\nand\\nTuning\\n: frompyspark.ml.evaluationimportMulticlassClassificationEvaluator;MulticlassClassificationEvaluator().evaluate(predictions)\\n14.StreamProcessing\\n●\\nReading\\nfrom\\na\\nStream\\n: dfStream=spark.readStream.format('source').load()●\\nWriting\\nto\\na\\nStream\\n: dfStream.writeStream.format('console').start()\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='2b893f8b-0d61-4c8e-9716-15b4fe5ee105', embedding=None, metadata={'page_label': '4', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"15.AdvancedDataExtraction\\n●\\nReading\\nfrom\\nMultiple\\nSources\\n: df=spark.read.format('format').option('option','value').load(['path1','path2'])●\\nIncremental\\nData\\nLoading\\n:\\nImplementing\\nlogic\\nto\\nload\\ndata\\nincrementally,\\nbased\\non\\ntimestamps\\nor\\nlog\\ntables.\\n16.ComplexDataTransformations\\n●\\nNested\\nJSON\\nParsing\\n: frompyspark.sql.functionsimportjson_tuple;df.select(json_tuple('json_column','field1','field2'))●\\nApplying\\nMap-Type\\nTransformations\\n:\\nUsing map\\nfunctions\\nto\\ntransform\\nkey-value\\npair\\ndata.\\n17.AdvancedJoinsandSetOperations\\n●\\nBroadcast\\nJoin\\nwith\\nLarge\\nand\\nSmall\\nDataFrames\\n:\\nUtilizingbroadcast\\nfor\\nefﬁcient\\njoins.●\\nSet\\nOperations\\n(Union,\\nIntersect,\\nExcept)\\n:\\nPerforming\\nset\\noperations\\nlike df1.union(df2)\\n, df1.intersect(df2)\\n,df1.except(df2)\\n.\\n18.DataAggregationandSummarization\\n●\\nComplex\\nAggregations\\n: df.groupBy('group_col').agg({'num_col1':'sum','num_col2':'avg'})●\\nRollup\\nand\\nCube\\nfor\\nMulti-Dimensional\\nAggregation\\n:df.rollup('col1','col2').sum()\\n, df.cube('col1','col2').mean()\\n19.AdvancedDataFiltering\\n●\\nFiltering\\nwith\\nComplex\\nConditions\\n: df.filter((df['col1']> value)&(df['col2']<other_value))●\\nUsing\\nColumn\\nExpressions\\n: frompyspark.sqlimportfunctionsasF;df.filter(F.col('col1').like('%pattern%'))\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='839a20c8-5c51-48e1-8c6a-63e3b8dc6619', embedding=None, metadata={'page_label': '5', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"20.WorkingwithDatesandTimes\\n●\\nDate\\nArithmetic\\n: df.withColumn('new_date',F.col('date_col')+F.expr('interval1 day'))●\\nDate\\nTruncation\\nand\\nFormatting\\n: df.withColumn('month',F.trunc('month','date_col'))\\n21.HandlingNestedandComplexStructures\\n●\\nWorking\\nwith\\nArrays\\nand\\nMaps\\n: df.select(F.explode('array_col'))\\n,df.select(F.col('map_col')['key'])●\\nFlattening\\nNested\\nStructures\\n: df.selectExpr('struct_col.*')\\n22.TextProcessingandNaturalLanguageProcessing\\n●\\nRegular\\nExpressions\\nfor\\nText\\nData\\n: df.withColumn('extracted',F.regexp_extract('text_col','(pattern)',1))●\\nSentiment\\nAnalysis\\non\\nText\\nData\\n:\\nUsing\\nNLP\\nlibraries\\nto\\nperform\\nsentiment\\nanalysis\\non\\ntextual\\ncolumns.\\n23.AdvancedWindowFunctions\\n●\\nWindow\\nFunctions\\nfor\\nRunning\\nTotals\\nand\\nMoving\\nAverages\\n: frompyspark.sql.windowimportWindow;windowSpec=Window.partitionBy('group_col').orderBy('date_col');df.withColumn('cumulative_sum',F.sum('num_col').over(windowSpec))●\\nRanking\\nand\\nRow\\nNumbering\\n: df.withColumn('rank',F.rank().over(windowSpec))\\n24.DataQualityandConsistencyChecks\\n●\\nData\\nProﬁling\\nfor\\nQuality\\nAssessment\\n:\\nGenerating\\nstatistics\\nfor\\neach\\ncolumn\\nto\\nassess\\ndata\\nquality.●\\nConsistency\\nChecks\\nAcross\\nDataFrames\\n:\\nComparing\\nschema\\nand\\nrow\\ncounts\\nbetween\\nDataFrames\\nfor\\nconsistency.\\n25.ETLPipelineMonitoringandLogging\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='c548a29a-9028-4a45-8e02-e01a5b96c419', embedding=None, metadata={'page_label': '6', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nImplementing\\nLogging\\nin\\nPySpark\\nJobs\\n:\\nUsing\\nPython's\\nlogging\\nmodule\\nto\\nlog\\nETL\\nprocess\\nsteps.●\\nMonitoring\\nPerformance\\nMetrics\\n:\\nTracking\\nexecution\\ntime\\nand\\nresource\\nutilization\\nof\\nETL\\njobs.\\n26.ETLWorkflowSchedulingandAutomation\\n●\\nIntegration\\nwith\\nWorkﬂow\\nManagement\\nTools\\n:\\nAutomating\\nPySpark\\nETL\\nscripts\\nusing\\ntools\\nlike\\nApache\\nAirﬂow\\nor\\nLuigi.●\\nScheduling\\nPeriodic\\nETL\\nJobs\\n:\\nSetting\\nup\\ncron\\njobs\\nor\\nusing\\nscheduler\\nservices\\nfor\\nregular\\nETL\\ntasks.\\n27.DataPartitioningandBucketing\\n●\\nPartitioning\\nData\\nfor\\nEfﬁcient\\nStorage\\n:df.write.partitionBy('date_col').parquet('path/to/output')●\\nBucketing\\nData\\nfor\\nOptimized\\nQuery\\nPerformance\\n:df.write.bucketBy(42,'key_col').sortBy('sort_col').saveAsTable('bucketed_table')\\n28.AdvancedSparkSQLTechniques\\n●\\nUsing\\nTemporary\\nViews\\nfor\\nSQL\\nQueries\\n:df.createOrReplaceTempView('temp_view');spark.sql('SELECT*FROMtemp_viewWHEREcol> value')●\\nComplex\\nSQL\\nQueries\\nfor\\nData\\nTransformation\\n:\\nUtilizing\\nadvanced\\nSQL\\nsyntax\\nfor\\ncomplex\\ndata\\ntransformations.\\n29.MachineLearningPipelines\\n●\\nCreating\\nand\\nTuning\\nML\\nPipelines\\n:\\nUsing\\nPySpark's\\nMLlib\\nfor\\nbuilding\\nand\\ntuning\\nmachine\\nlearning\\npipelines.●\\nFeature\\nEngineering\\nin\\nML\\nPipelines\\n:\\nImplementing\\nfeature\\ntransformers\\nand\\nselectors\\nwithin\\nML\\npipelines.\\n30.IntegrationwithOtherBigDataTools\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='13989c90-6db1-40cb-9a8f-a98249f63d47', embedding=None, metadata={'page_label': '7', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='●\\nReading\\nand\\nWriting\\nData\\nto\\nHDFS\\n:\\nAccessing\\nHadoop\\nDistributed\\nFile\\nSystem\\n(HDFS)\\nfor\\ndata\\nstorage\\nand\\nretrieval.●\\nInterfacing\\nwith\\nKafka\\nfor\\nReal-Time\\nData\\nProcessing\\n:\\nConnecting\\nto\\nApache\\nKafka\\nfor\\nstream\\nprocessing\\ntasks.\\n31.Cloud-SpecificPySparkOperations\\n●\\nUtilizing\\nCloud-Speciﬁc\\nStorage\\nOptions\\n:\\nLeveraging\\nAWS\\nS3,\\nAzure\\nBlob\\nStorage,\\nor\\nGCP\\nStorage\\nin\\nPySpark.●\\nCloud-Based\\nData\\nProcessing\\nServices\\nIntegration\\n:\\nUsing\\nservices\\nlike\\nAWS\\nGlue\\nor\\nAzure\\nSynapse\\nfor\\nETL\\nprocesses.\\n32.SecurityandComplianceinETL\\n●\\nImplementing\\nData\\nEncryption\\nand\\nSecurity\\n:\\nSecuring\\ndata\\nat\\nrest\\nand\\nin\\ntransit\\nduring\\nETL\\nprocesses.●\\nCompliance\\nwith\\nData\\nProtection\\nRegulations\\n:\\nAdhering\\nto\\nGDPR,\\nHIPAA,\\nor\\nother\\nregulations\\nin\\ndata\\nprocessing.\\n33.OptimizingETLProcessesforScalability\\n●\\nDynamic\\nResource\\nAllocation\\nfor\\nETL\\nJobs\\n:\\nAdjusting\\nSpark\\nconﬁgurations\\nfor\\noptimal\\nresource\\nusage.●\\nBest\\nPractices\\nfor\\nScaling\\nETL\\nProcesses\\n:\\nTechniques\\nfor\\nscaling\\nETL\\npipelines\\nto\\nhandle\\ngrowing\\ndata\\nvolumes.\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0901609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.text_template= \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"page_label\")\n",
    "\n",
    "\n",
    "# file path also is not crucial\n",
    "for doc in docs:\n",
    "    doc.text_template= \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "    if \"file_path\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bf741af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='0dcc0b80-1a85-47af-8fc5-db7abeb0a9b3', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[ \\nE T L \\np r o c e s s e s \\nu s i n g \\nP y S p a r k \\n] \\n# \\nQ u i c k \\nS u m m a r y \\n 1.EnvironmentSetupandSparkSessionCreation\\n●\\nInstall\\nPySpark\\n: pipinstallpyspark●\\nStart\\na\\nSparkSession\\n: frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName(\\'ETLProcess\\').getOrCreate()\\n2.DataExtraction\\n●\\nRead\\nData\\nfrom\\nCSV\\n: df= spark.read.csv(\\'path/to/csv\\',inferSchema=True,header=True)●\\nRead\\nData\\nfrom\\nJSON\\n: df= spark.read.json(\\'path/to/json\\')●\\nRead\\nData\\nfrom\\nParquet\\n: df= spark.read.parquet(\\'path/to/parquet\\')●\\nRead\\nData\\nfrom\\na\\nDatabase\\n: df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\\n3.DataTransformation\\n●\\nSelecting\\nColumns\\n: df.select(\\'column1\\',\\'column2\\')●\\nFiltering\\nData\\n: df.filter(df[\\'column\\']> value)●\\nAdding\\nNew\\nColumns\\n: df.withColumn(\\'new_column\\',df[\\'column\\']+10)●\\nRenaming\\nColumns\\n: df.withColumnRenamed(\\'old_name\\',\\'new_name\\')●\\nGrouping\\nand\\nAggregating\\nData\\n: df.groupBy(\\'column\\').agg({\\'column2\\':\\'sum\\'})●\\nJoining\\nDataFrames\\n: df1.join(df2,df1[\\'id\\']==df2[\\'id\\'])●\\nSorting\\nData\\n: df.orderBy(df[\\'column\\'].desc())●\\nRemoving\\nDuplicates\\n: df.dropDuplicates()\\n4.HandlingMissingValues\\n●\\nDropping\\nRows\\nwith\\nMissing\\nValues\\n: df.na.drop()●\\nFilling\\nMissing\\nValues\\n: df.na.fill(value)●\\nReplacing\\nValues\\n: df.na.replace([\\'old_value\\'],[\\'new_value\\'])\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='24b4264f-dfba-4654-9c92-29fa059a2b97', embedding=None, metadata={'page_label': '2', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.DataTypeConversion\\n●\\nChanging\\nColumn\\nTypes\\n: df.withColumn(\\'column\\',df[\\'column\\'].cast(\\'new_type\\'))●\\nParsing\\nDates\\n: frompyspark.sql.functionsimportto_date;df.withColumn(\\'date\\',to_date(df[\\'date_string\\']))\\n6.AdvancedDataManipulations\\n●\\nUsing\\nSQL\\nQueries\\n: df.createOrReplaceTempView(\\'table\\');spark.sql(\\'SELECT*FROMtableWHEREcolumn> value\\')●\\nWindow\\nFunctions\\n: frompyspark.sql.windowimportWindow;frompyspark.sql.functionsimportrow_number;df.withColumn(\\'row\\',row_number().over(Window.partitionBy(\\'column\\').orderBy(\\'other_column\\')))●\\nPivot\\nTables\\n:df.groupBy(\\'column\\').pivot(\\'pivot_column\\').agg({\\'column2\\':\\'sum\\'})\\n7.DataLoading\\n●\\nWriting\\nto\\nCSV\\n: df.write.csv(\\'path/to/output\\')●\\nWriting\\nto\\nJSON\\n: df.write.json(\\'path/to/output\\')●\\nWriting\\nto\\nParquet\\n: df.write.parquet(\\'path/to/output\\')●\\nWriting\\nto\\na\\nDatabase\\n: df.write.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").save()\\n8.PerformanceTuning\\n●\\nCaching\\nData\\n: df.cache()●\\nBroadcasting\\na\\nDataFrame\\nfor\\nJoin\\nOptimization\\n: frompyspark.sql.functionsimportbroadcast;df1.join(broadcast(df2),df1[\\'id\\']==df2[\\'id\\'])●\\nRepartitioning\\nData\\n: df.repartition(10)●\\nCoalescing\\nPartitions\\n: df.coalesce(1)\\n9.DebuggingandErrorHandling\\n●\\nShowing\\nExecution\\nPlan\\n: df.explain() \\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='bc54acb6-6ec0-4d4e-9a1b-0f9ea69f7e23', embedding=None, metadata={'page_label': '3', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nCatching\\nExceptions\\nduring\\nRead\\n:\\nImplement\\ntry-except\\nblocks\\nduring\\ndata\\nreading\\noperations.\\n10.WorkingwithComplexDataTypes\\n●\\nExploding\\nArrays\\n: frompyspark.sql.functionsimportexplode;df.select(explode(df['array_column']))●\\nHandling\\nStruct\\nFields\\n: df.select('struct_column.field1','struct_column.field2')\\n11.CustomTransformationswithUDFs\\n●\\nDeﬁning\\na\\nUDF\\n: frompyspark.sql.functionsimportudf;@udf('return_type')defmy_udf(column):returntransformation●\\nApplying\\nUDF\\non\\nDataFrame\\n: df.withColumn('new_column',my_udf(df['column']))\\n12.WorkingwithLargeTextData\\n●\\nTokenizing\\nText\\nData\\n: frompyspark.ml.featureimportTokenizer;Tokenizer(inputCol='text_column',outputCol='words').transform(df)●\\nTF-IDF\\non\\nText\\nData\\n: frompyspark.ml.featureimportHashingTF,IDF;HashingTF(inputCol='words',outputCol='rawFeatures').transform(df)\\n13.MachineLearningIntegration\\n●\\nUsing\\nMLlib\\nfor\\nPredictive\\nModeling\\n:\\nBuilding\\nand\\ntraining\\nmachine\\nlearning\\nmodels\\nusing\\nPySpark's\\nMLlib.●\\nModel\\nEvaluation\\nand\\nTuning\\n: frompyspark.ml.evaluationimportMulticlassClassificationEvaluator;MulticlassClassificationEvaluator().evaluate(predictions)\\n14.StreamProcessing\\n●\\nReading\\nfrom\\na\\nStream\\n: dfStream=spark.readStream.format('source').load()●\\nWriting\\nto\\na\\nStream\\n: dfStream.writeStream.format('console').start()\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='2b893f8b-0d61-4c8e-9716-15b4fe5ee105', embedding=None, metadata={'page_label': '4', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"15.AdvancedDataExtraction\\n●\\nReading\\nfrom\\nMultiple\\nSources\\n: df=spark.read.format('format').option('option','value').load(['path1','path2'])●\\nIncremental\\nData\\nLoading\\n:\\nImplementing\\nlogic\\nto\\nload\\ndata\\nincrementally,\\nbased\\non\\ntimestamps\\nor\\nlog\\ntables.\\n16.ComplexDataTransformations\\n●\\nNested\\nJSON\\nParsing\\n: frompyspark.sql.functionsimportjson_tuple;df.select(json_tuple('json_column','field1','field2'))●\\nApplying\\nMap-Type\\nTransformations\\n:\\nUsing map\\nfunctions\\nto\\ntransform\\nkey-value\\npair\\ndata.\\n17.AdvancedJoinsandSetOperations\\n●\\nBroadcast\\nJoin\\nwith\\nLarge\\nand\\nSmall\\nDataFrames\\n:\\nUtilizingbroadcast\\nfor\\nefﬁcient\\njoins.●\\nSet\\nOperations\\n(Union,\\nIntersect,\\nExcept)\\n:\\nPerforming\\nset\\noperations\\nlike df1.union(df2)\\n, df1.intersect(df2)\\n,df1.except(df2)\\n.\\n18.DataAggregationandSummarization\\n●\\nComplex\\nAggregations\\n: df.groupBy('group_col').agg({'num_col1':'sum','num_col2':'avg'})●\\nRollup\\nand\\nCube\\nfor\\nMulti-Dimensional\\nAggregation\\n:df.rollup('col1','col2').sum()\\n, df.cube('col1','col2').mean()\\n19.AdvancedDataFiltering\\n●\\nFiltering\\nwith\\nComplex\\nConditions\\n: df.filter((df['col1']> value)&(df['col2']<other_value))●\\nUsing\\nColumn\\nExpressions\\n: frompyspark.sqlimportfunctionsasF;df.filter(F.col('col1').like('%pattern%'))\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='839a20c8-5c51-48e1-8c6a-63e3b8dc6619', embedding=None, metadata={'page_label': '5', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"20.WorkingwithDatesandTimes\\n●\\nDate\\nArithmetic\\n: df.withColumn('new_date',F.col('date_col')+F.expr('interval1 day'))●\\nDate\\nTruncation\\nand\\nFormatting\\n: df.withColumn('month',F.trunc('month','date_col'))\\n21.HandlingNestedandComplexStructures\\n●\\nWorking\\nwith\\nArrays\\nand\\nMaps\\n: df.select(F.explode('array_col'))\\n,df.select(F.col('map_col')['key'])●\\nFlattening\\nNested\\nStructures\\n: df.selectExpr('struct_col.*')\\n22.TextProcessingandNaturalLanguageProcessing\\n●\\nRegular\\nExpressions\\nfor\\nText\\nData\\n: df.withColumn('extracted',F.regexp_extract('text_col','(pattern)',1))●\\nSentiment\\nAnalysis\\non\\nText\\nData\\n:\\nUsing\\nNLP\\nlibraries\\nto\\nperform\\nsentiment\\nanalysis\\non\\ntextual\\ncolumns.\\n23.AdvancedWindowFunctions\\n●\\nWindow\\nFunctions\\nfor\\nRunning\\nTotals\\nand\\nMoving\\nAverages\\n: frompyspark.sql.windowimportWindow;windowSpec=Window.partitionBy('group_col').orderBy('date_col');df.withColumn('cumulative_sum',F.sum('num_col').over(windowSpec))●\\nRanking\\nand\\nRow\\nNumbering\\n: df.withColumn('rank',F.rank().over(windowSpec))\\n24.DataQualityandConsistencyChecks\\n●\\nData\\nProﬁling\\nfor\\nQuality\\nAssessment\\n:\\nGenerating\\nstatistics\\nfor\\neach\\ncolumn\\nto\\nassess\\ndata\\nquality.●\\nConsistency\\nChecks\\nAcross\\nDataFrames\\n:\\nComparing\\nschema\\nand\\nrow\\ncounts\\nbetween\\nDataFrames\\nfor\\nconsistency.\\n25.ETLPipelineMonitoringandLogging\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='c548a29a-9028-4a45-8e02-e01a5b96c419', embedding=None, metadata={'page_label': '6', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nImplementing\\nLogging\\nin\\nPySpark\\nJobs\\n:\\nUsing\\nPython's\\nlogging\\nmodule\\nto\\nlog\\nETL\\nprocess\\nsteps.●\\nMonitoring\\nPerformance\\nMetrics\\n:\\nTracking\\nexecution\\ntime\\nand\\nresource\\nutilization\\nof\\nETL\\njobs.\\n26.ETLWorkflowSchedulingandAutomation\\n●\\nIntegration\\nwith\\nWorkﬂow\\nManagement\\nTools\\n:\\nAutomating\\nPySpark\\nETL\\nscripts\\nusing\\ntools\\nlike\\nApache\\nAirﬂow\\nor\\nLuigi.●\\nScheduling\\nPeriodic\\nETL\\nJobs\\n:\\nSetting\\nup\\ncron\\njobs\\nor\\nusing\\nscheduler\\nservices\\nfor\\nregular\\nETL\\ntasks.\\n27.DataPartitioningandBucketing\\n●\\nPartitioning\\nData\\nfor\\nEfﬁcient\\nStorage\\n:df.write.partitionBy('date_col').parquet('path/to/output')●\\nBucketing\\nData\\nfor\\nOptimized\\nQuery\\nPerformance\\n:df.write.bucketBy(42,'key_col').sortBy('sort_col').saveAsTable('bucketed_table')\\n28.AdvancedSparkSQLTechniques\\n●\\nUsing\\nTemporary\\nViews\\nfor\\nSQL\\nQueries\\n:df.createOrReplaceTempView('temp_view');spark.sql('SELECT*FROMtemp_viewWHEREcol> value')●\\nComplex\\nSQL\\nQueries\\nfor\\nData\\nTransformation\\n:\\nUtilizing\\nadvanced\\nSQL\\nsyntax\\nfor\\ncomplex\\ndata\\ntransformations.\\n29.MachineLearningPipelines\\n●\\nCreating\\nand\\nTuning\\nML\\nPipelines\\n:\\nUsing\\nPySpark's\\nMLlib\\nfor\\nbuilding\\nand\\ntuning\\nmachine\\nlearning\\npipelines.●\\nFeature\\nEngineering\\nin\\nML\\nPipelines\\n:\\nImplementing\\nfeature\\ntransformers\\nand\\nselectors\\nwithin\\nML\\npipelines.\\n30.IntegrationwithOtherBigDataTools\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='13989c90-6db1-40cb-9a8f-a98249f63d47', embedding=None, metadata={'page_label': '7', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='●\\nReading\\nand\\nWriting\\nData\\nto\\nHDFS\\n:\\nAccessing\\nHadoop\\nDistributed\\nFile\\nSystem\\n(HDFS)\\nfor\\ndata\\nstorage\\nand\\nretrieval.●\\nInterfacing\\nwith\\nKafka\\nfor\\nReal-Time\\nData\\nProcessing\\n:\\nConnecting\\nto\\nApache\\nKafka\\nfor\\nstream\\nprocessing\\ntasks.\\n31.Cloud-SpecificPySparkOperations\\n●\\nUtilizing\\nCloud-Speciﬁc\\nStorage\\nOptions\\n:\\nLeveraging\\nAWS\\nS3,\\nAzure\\nBlob\\nStorage,\\nor\\nGCP\\nStorage\\nin\\nPySpark.●\\nCloud-Based\\nData\\nProcessing\\nServices\\nIntegration\\n:\\nUsing\\nservices\\nlike\\nAWS\\nGlue\\nor\\nAzure\\nSynapse\\nfor\\nETL\\nprocesses.\\n32.SecurityandComplianceinETL\\n●\\nImplementing\\nData\\nEncryption\\nand\\nSecurity\\n:\\nSecuring\\ndata\\nat\\nrest\\nand\\nin\\ntransit\\nduring\\nETL\\nprocesses.●\\nCompliance\\nwith\\nData\\nProtection\\nRegulations\\n:\\nAdhering\\nto\\nGDPR,\\nHIPAA,\\nor\\nother\\nregulations\\nin\\ndata\\nprocessing.\\n33.OptimizingETLProcessesforScalability\\n●\\nDynamic\\nResource\\nAllocation\\nfor\\nETL\\nJobs\\n:\\nAdjusting\\nSpark\\nconﬁgurations\\nfor\\noptimal\\nresource\\nusage.●\\nBest\\nPractices\\nfor\\nScaling\\nETL\\nProcesses\\n:\\nTechniques\\nfor\\nscaling\\nETL\\npipelines\\nto\\nhandle\\ngrowing\\ndata\\nvolumes.\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ca4745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# somtimes you embed the metadata, page number is probalbly not meant for embedding\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17ffc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_path: c:\\Users\\Ibrahim\\Documents\\WORK\\Faculty-Projects\\nlp\\notebook\\..\\data\\Transformations in pyspark .pdf\n",
      "\n",
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.LLM))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bbe786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document= Document(text= \"This framework is amazing!\",\n",
    "                   metadata= {\n",
    "                       \"filename\": \"spark the definitive guide\",\n",
    "                       \"category\": \"technology\",\n",
    "                       \"author\": \"ibrahim\"\n",
    "                   },\n",
    "                   excluded_llm_metadata_keys= [\"filename\"],\n",
    "                   excluded_embed_metadata_keys= [\"filename\"],\n",
    "                   metadata_separator= \"\\n\",\n",
    "                   metadata_template= \"{key}=>{value}\",\n",
    "                   text_template= \"Metadata: \\n{metadata_str}\\n-------\\n Content=>{content}\"\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8775cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The llm see this \n",
      " Metadata: \n",
      "category=>technology\n",
      "author=>ibrahim\n",
      "-------\n",
      " Content=>This framework is amazing!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The llm see this \\n\",\n",
    "      document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2e5a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding see this \n",
      " Metadata: category:technology\n",
      "author:ibrahim\n",
      " --------\n",
      " Content: This framework is amazing!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The embedding see this \\n\",\n",
    "      document.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROK_API_KEY\"]= getpass.getpass(\"Enter the api key si ibrahim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_transformations= Groq(model= \"qwen-2.5-32b\", api_key=os.environ[\"GROK_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d949cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter= SentenceSplitter(separator=\" \", chunk_size=1024, chunk_overlap=128)\n",
    "title_extractor= TitleExtractor(llm=llm_transformations, nodes=5)\n",
    "qa_extractor= QuestionsAnsweredExtractor(llm=llm_transformations, questions= 3)\n",
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline= IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor,\n",
    "        qa_extractor\n",
    "    ]\n",
    ")\n",
    "nodes= pipeline.run(\n",
    "    documents= docs,\n",
    "    in_place= True,\n",
    "    show_progress= True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
