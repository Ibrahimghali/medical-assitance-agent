{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beed4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex RAG Pipeline Exploration\n",
    "\n",
    "# This notebook explores the LlamaIndex framework for building a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "## Table of Contents\n",
    "# 1. Setup and Imports\n",
    "# 2. Document Loading\n",
    "# 3. Document Metadata Configuration\n",
    "# 4. Document Transformation & Extraction\n",
    "# 5. Embedding Generation\n",
    "# 6. Vector Store and Indexing\n",
    "# 7. Query Engine\n",
    "# 8. Persistent Storage with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecd766",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42cbbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for document processing and LLM integration\n",
    "from llama_index.core import SimpleDirectoryReader, Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor, QuestionsAnsweredExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# LLM and embedding imports\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Vector store imports\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "# Utility imports\n",
    "import os\n",
    "import getpass\n",
    "import pprint\n",
    "\n",
    "# Fix for nested asyncio in Jupyter notebooks - MUST be applied before any LlamaIndex operations\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe331b5",
   "metadata": {},
   "source": [
    "## 2. Document Loading\n",
    "\n",
    "Load documents from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c684ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents from the data directory\n",
    "# filename_as_id=False means auto-generate document IDs\n",
    "docs = SimpleDirectoryReader(input_dir='../data', filename_as_id=False).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8d7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 7\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents loaded\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b833a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='911b995b-07a3-4c54-8cbc-7ea1cebe5e03', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[ \\nE T L \\np r o c e s s e s \\nu s i n g \\nP y S p a r k \\n] \\n# \\nQ u i c k \\nS u m m a r y \\n 1.EnvironmentSetupandSparkSessionCreation\\n●\\nInstall\\nPySpark\\n: pipinstallpyspark●\\nStart\\na\\nSparkSession\\n: frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName(\\'ETLProcess\\').getOrCreate()\\n2.DataExtraction\\n●\\nRead\\nData\\nfrom\\nCSV\\n: df= spark.read.csv(\\'path/to/csv\\',inferSchema=True,header=True)●\\nRead\\nData\\nfrom\\nJSON\\n: df= spark.read.json(\\'path/to/json\\')●\\nRead\\nData\\nfrom\\nParquet\\n: df= spark.read.parquet(\\'path/to/parquet\\')●\\nRead\\nData\\nfrom\\na\\nDatabase\\n: df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\\n3.DataTransformation\\n●\\nSelecting\\nColumns\\n: df.select(\\'column1\\',\\'column2\\')●\\nFiltering\\nData\\n: df.filter(df[\\'column\\']> value)●\\nAdding\\nNew\\nColumns\\n: df.withColumn(\\'new_column\\',df[\\'column\\']+10)●\\nRenaming\\nColumns\\n: df.withColumnRenamed(\\'old_name\\',\\'new_name\\')●\\nGrouping\\nand\\nAggregating\\nData\\n: df.groupBy(\\'column\\').agg({\\'column2\\':\\'sum\\'})●\\nJoining\\nDataFrames\\n: df1.join(df2,df1[\\'id\\']==df2[\\'id\\'])●\\nSorting\\nData\\n: df.orderBy(df[\\'column\\'].desc())●\\nRemoving\\nDuplicates\\n: df.dropDuplicates()\\n4.HandlingMissingValues\\n●\\nDropping\\nRows\\nwith\\nMissing\\nValues\\n: df.na.drop()●\\nFilling\\nMissing\\nValues\\n: df.na.fill(value)●\\nReplacing\\nValues\\n: df.na.replace([\\'old_value\\'],[\\'new_value\\'])\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='5bfa90a0-bc32-48af-9abb-3929122e0bbe', embedding=None, metadata={'page_label': '2', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.DataTypeConversion\\n●\\nChanging\\nColumn\\nTypes\\n: df.withColumn(\\'column\\',df[\\'column\\'].cast(\\'new_type\\'))●\\nParsing\\nDates\\n: frompyspark.sql.functionsimportto_date;df.withColumn(\\'date\\',to_date(df[\\'date_string\\']))\\n6.AdvancedDataManipulations\\n●\\nUsing\\nSQL\\nQueries\\n: df.createOrReplaceTempView(\\'table\\');spark.sql(\\'SELECT*FROMtableWHEREcolumn> value\\')●\\nWindow\\nFunctions\\n: frompyspark.sql.windowimportWindow;frompyspark.sql.functionsimportrow_number;df.withColumn(\\'row\\',row_number().over(Window.partitionBy(\\'column\\').orderBy(\\'other_column\\')))●\\nPivot\\nTables\\n:df.groupBy(\\'column\\').pivot(\\'pivot_column\\').agg({\\'column2\\':\\'sum\\'})\\n7.DataLoading\\n●\\nWriting\\nto\\nCSV\\n: df.write.csv(\\'path/to/output\\')●\\nWriting\\nto\\nJSON\\n: df.write.json(\\'path/to/output\\')●\\nWriting\\nto\\nParquet\\n: df.write.parquet(\\'path/to/output\\')●\\nWriting\\nto\\na\\nDatabase\\n: df.write.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").save()\\n8.PerformanceTuning\\n●\\nCaching\\nData\\n: df.cache()●\\nBroadcasting\\na\\nDataFrame\\nfor\\nJoin\\nOptimization\\n: frompyspark.sql.functionsimportbroadcast;df1.join(broadcast(df2),df1[\\'id\\']==df2[\\'id\\'])●\\nRepartitioning\\nData\\n: df.repartition(10)●\\nCoalescing\\nPartitions\\n: df.coalesce(1)\\n9.DebuggingandErrorHandling\\n●\\nShowing\\nExecution\\nPlan\\n: df.explain() \\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='29ec1856-6de2-4519-8976-e74b1480ec38', embedding=None, metadata={'page_label': '3', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nCatching\\nExceptions\\nduring\\nRead\\n:\\nImplement\\ntry-except\\nblocks\\nduring\\ndata\\nreading\\noperations.\\n10.WorkingwithComplexDataTypes\\n●\\nExploding\\nArrays\\n: frompyspark.sql.functionsimportexplode;df.select(explode(df['array_column']))●\\nHandling\\nStruct\\nFields\\n: df.select('struct_column.field1','struct_column.field2')\\n11.CustomTransformationswithUDFs\\n●\\nDeﬁning\\na\\nUDF\\n: frompyspark.sql.functionsimportudf;@udf('return_type')defmy_udf(column):returntransformation●\\nApplying\\nUDF\\non\\nDataFrame\\n: df.withColumn('new_column',my_udf(df['column']))\\n12.WorkingwithLargeTextData\\n●\\nTokenizing\\nText\\nData\\n: frompyspark.ml.featureimportTokenizer;Tokenizer(inputCol='text_column',outputCol='words').transform(df)●\\nTF-IDF\\non\\nText\\nData\\n: frompyspark.ml.featureimportHashingTF,IDF;HashingTF(inputCol='words',outputCol='rawFeatures').transform(df)\\n13.MachineLearningIntegration\\n●\\nUsing\\nMLlib\\nfor\\nPredictive\\nModeling\\n:\\nBuilding\\nand\\ntraining\\nmachine\\nlearning\\nmodels\\nusing\\nPySpark's\\nMLlib.●\\nModel\\nEvaluation\\nand\\nTuning\\n: frompyspark.ml.evaluationimportMulticlassClassificationEvaluator;MulticlassClassificationEvaluator().evaluate(predictions)\\n14.StreamProcessing\\n●\\nReading\\nfrom\\na\\nStream\\n: dfStream=spark.readStream.format('source').load()●\\nWriting\\nto\\na\\nStream\\n: dfStream.writeStream.format('console').start()\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='3fc0f47b-56a8-4865-b432-ef00b76070bd', embedding=None, metadata={'page_label': '4', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"15.AdvancedDataExtraction\\n●\\nReading\\nfrom\\nMultiple\\nSources\\n: df=spark.read.format('format').option('option','value').load(['path1','path2'])●\\nIncremental\\nData\\nLoading\\n:\\nImplementing\\nlogic\\nto\\nload\\ndata\\nincrementally,\\nbased\\non\\ntimestamps\\nor\\nlog\\ntables.\\n16.ComplexDataTransformations\\n●\\nNested\\nJSON\\nParsing\\n: frompyspark.sql.functionsimportjson_tuple;df.select(json_tuple('json_column','field1','field2'))●\\nApplying\\nMap-Type\\nTransformations\\n:\\nUsing map\\nfunctions\\nto\\ntransform\\nkey-value\\npair\\ndata.\\n17.AdvancedJoinsandSetOperations\\n●\\nBroadcast\\nJoin\\nwith\\nLarge\\nand\\nSmall\\nDataFrames\\n:\\nUtilizingbroadcast\\nfor\\nefﬁcient\\njoins.●\\nSet\\nOperations\\n(Union,\\nIntersect,\\nExcept)\\n:\\nPerforming\\nset\\noperations\\nlike df1.union(df2)\\n, df1.intersect(df2)\\n,df1.except(df2)\\n.\\n18.DataAggregationandSummarization\\n●\\nComplex\\nAggregations\\n: df.groupBy('group_col').agg({'num_col1':'sum','num_col2':'avg'})●\\nRollup\\nand\\nCube\\nfor\\nMulti-Dimensional\\nAggregation\\n:df.rollup('col1','col2').sum()\\n, df.cube('col1','col2').mean()\\n19.AdvancedDataFiltering\\n●\\nFiltering\\nwith\\nComplex\\nConditions\\n: df.filter((df['col1']> value)&(df['col2']<other_value))●\\nUsing\\nColumn\\nExpressions\\n: frompyspark.sqlimportfunctionsasF;df.filter(F.col('col1').like('%pattern%'))\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='ff5d9526-435f-4b67-9021-53b8237569ae', embedding=None, metadata={'page_label': '5', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"20.WorkingwithDatesandTimes\\n●\\nDate\\nArithmetic\\n: df.withColumn('new_date',F.col('date_col')+F.expr('interval1 day'))●\\nDate\\nTruncation\\nand\\nFormatting\\n: df.withColumn('month',F.trunc('month','date_col'))\\n21.HandlingNestedandComplexStructures\\n●\\nWorking\\nwith\\nArrays\\nand\\nMaps\\n: df.select(F.explode('array_col'))\\n,df.select(F.col('map_col')['key'])●\\nFlattening\\nNested\\nStructures\\n: df.selectExpr('struct_col.*')\\n22.TextProcessingandNaturalLanguageProcessing\\n●\\nRegular\\nExpressions\\nfor\\nText\\nData\\n: df.withColumn('extracted',F.regexp_extract('text_col','(pattern)',1))●\\nSentiment\\nAnalysis\\non\\nText\\nData\\n:\\nUsing\\nNLP\\nlibraries\\nto\\nperform\\nsentiment\\nanalysis\\non\\ntextual\\ncolumns.\\n23.AdvancedWindowFunctions\\n●\\nWindow\\nFunctions\\nfor\\nRunning\\nTotals\\nand\\nMoving\\nAverages\\n: frompyspark.sql.windowimportWindow;windowSpec=Window.partitionBy('group_col').orderBy('date_col');df.withColumn('cumulative_sum',F.sum('num_col').over(windowSpec))●\\nRanking\\nand\\nRow\\nNumbering\\n: df.withColumn('rank',F.rank().over(windowSpec))\\n24.DataQualityandConsistencyChecks\\n●\\nData\\nProﬁling\\nfor\\nQuality\\nAssessment\\n:\\nGenerating\\nstatistics\\nfor\\neach\\ncolumn\\nto\\nassess\\ndata\\nquality.●\\nConsistency\\nChecks\\nAcross\\nDataFrames\\n:\\nComparing\\nschema\\nand\\nrow\\ncounts\\nbetween\\nDataFrames\\nfor\\nconsistency.\\n25.ETLPipelineMonitoringandLogging\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='a72fda39-61f0-48f8-8e51-1d6ba9afcfd9', embedding=None, metadata={'page_label': '6', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nImplementing\\nLogging\\nin\\nPySpark\\nJobs\\n:\\nUsing\\nPython's\\nlogging\\nmodule\\nto\\nlog\\nETL\\nprocess\\nsteps.●\\nMonitoring\\nPerformance\\nMetrics\\n:\\nTracking\\nexecution\\ntime\\nand\\nresource\\nutilization\\nof\\nETL\\njobs.\\n26.ETLWorkflowSchedulingandAutomation\\n●\\nIntegration\\nwith\\nWorkﬂow\\nManagement\\nTools\\n:\\nAutomating\\nPySpark\\nETL\\nscripts\\nusing\\ntools\\nlike\\nApache\\nAirﬂow\\nor\\nLuigi.●\\nScheduling\\nPeriodic\\nETL\\nJobs\\n:\\nSetting\\nup\\ncron\\njobs\\nor\\nusing\\nscheduler\\nservices\\nfor\\nregular\\nETL\\ntasks.\\n27.DataPartitioningandBucketing\\n●\\nPartitioning\\nData\\nfor\\nEfﬁcient\\nStorage\\n:df.write.partitionBy('date_col').parquet('path/to/output')●\\nBucketing\\nData\\nfor\\nOptimized\\nQuery\\nPerformance\\n:df.write.bucketBy(42,'key_col').sortBy('sort_col').saveAsTable('bucketed_table')\\n28.AdvancedSparkSQLTechniques\\n●\\nUsing\\nTemporary\\nViews\\nfor\\nSQL\\nQueries\\n:df.createOrReplaceTempView('temp_view');spark.sql('SELECT*FROMtemp_viewWHEREcol> value')●\\nComplex\\nSQL\\nQueries\\nfor\\nData\\nTransformation\\n:\\nUtilizing\\nadvanced\\nSQL\\nsyntax\\nfor\\ncomplex\\ndata\\ntransformations.\\n29.MachineLearningPipelines\\n●\\nCreating\\nand\\nTuning\\nML\\nPipelines\\n:\\nUsing\\nPySpark's\\nMLlib\\nfor\\nbuilding\\nand\\ntuning\\nmachine\\nlearning\\npipelines.●\\nFeature\\nEngineering\\nin\\nML\\nPipelines\\n:\\nImplementing\\nfeature\\ntransformers\\nand\\nselectors\\nwithin\\nML\\npipelines.\\n30.IntegrationwithOtherBigDataTools\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
      " Document(id_='cd7a7ce4-814a-405a-9796-1edf083cdee2', embedding=None, metadata={'page_label': '7', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='●\\nReading\\nand\\nWriting\\nData\\nto\\nHDFS\\n:\\nAccessing\\nHadoop\\nDistributed\\nFile\\nSystem\\n(HDFS)\\nfor\\ndata\\nstorage\\nand\\nretrieval.●\\nInterfacing\\nwith\\nKafka\\nfor\\nReal-Time\\nData\\nProcessing\\n:\\nConnecting\\nto\\nApache\\nKafka\\nfor\\nstream\\nprocessing\\ntasks.\\n31.Cloud-SpecificPySparkOperations\\n●\\nUtilizing\\nCloud-Speciﬁc\\nStorage\\nOptions\\n:\\nLeveraging\\nAWS\\nS3,\\nAzure\\nBlob\\nStorage,\\nor\\nGCP\\nStorage\\nin\\nPySpark.●\\nCloud-Based\\nData\\nProcessing\\nServices\\nIntegration\\n:\\nUsing\\nservices\\nlike\\nAWS\\nGlue\\nor\\nAzure\\nSynapse\\nfor\\nETL\\nprocesses.\\n32.SecurityandComplianceinETL\\n●\\nImplementing\\nData\\nEncryption\\nand\\nSecurity\\n:\\nSecuring\\ndata\\nat\\nrest\\nand\\nin\\ntransit\\nduring\\nETL\\nprocesses.●\\nCompliance\\nwith\\nData\\nProtection\\nRegulations\\n:\\nAdhering\\nto\\nGDPR,\\nHIPAA,\\nor\\nother\\nregulations\\nin\\ndata\\nprocessing.\\n33.OptimizingETLProcessesforScalability\\n●\\nDynamic\\nResource\\nAllocation\\nfor\\nETL\\nJobs\\n:\\nAdjusting\\nSpark\\nconﬁgurations\\nfor\\noptimal\\nresource\\nusage.●\\nBest\\nPractices\\nfor\\nScaling\\nETL\\nProcesses\\n:\\nTechniques\\nfor\\nscaling\\nETL\\npipelines\\nto\\nhandle\\ngrowing\\ndata\\nvolumes.\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the loaded documents structure\n",
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172d7ec",
   "metadata": {},
   "source": [
    "## 3. Document Metadata Configuration\n",
    "\n",
    "Configure which metadata should be included when sending documents to LLMs vs embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0901609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure document text templates and metadata exclusions\n",
    "# page_label is not useful for embeddings, so we exclude it\n",
    "for doc in docs:\n",
    "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"page_label\")\n",
    "\n",
    "# file_path is also not crucial for embeddings\n",
    "for doc in docs:\n",
    "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "    if \"file_path\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf741af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='911b995b-07a3-4c54-8cbc-7ea1cebe5e03', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[ \\nE T L \\np r o c e s s e s \\nu s i n g \\nP y S p a r k \\n] \\n# \\nQ u i c k \\nS u m m a r y \\n 1.EnvironmentSetupandSparkSessionCreation\\n●\\nInstall\\nPySpark\\n: pipinstallpyspark●\\nStart\\na\\nSparkSession\\n: frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName(\\'ETLProcess\\').getOrCreate()\\n2.DataExtraction\\n●\\nRead\\nData\\nfrom\\nCSV\\n: df= spark.read.csv(\\'path/to/csv\\',inferSchema=True,header=True)●\\nRead\\nData\\nfrom\\nJSON\\n: df= spark.read.json(\\'path/to/json\\')●\\nRead\\nData\\nfrom\\nParquet\\n: df= spark.read.parquet(\\'path/to/parquet\\')●\\nRead\\nData\\nfrom\\na\\nDatabase\\n: df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\\n3.DataTransformation\\n●\\nSelecting\\nColumns\\n: df.select(\\'column1\\',\\'column2\\')●\\nFiltering\\nData\\n: df.filter(df[\\'column\\']> value)●\\nAdding\\nNew\\nColumns\\n: df.withColumn(\\'new_column\\',df[\\'column\\']+10)●\\nRenaming\\nColumns\\n: df.withColumnRenamed(\\'old_name\\',\\'new_name\\')●\\nGrouping\\nand\\nAggregating\\nData\\n: df.groupBy(\\'column\\').agg({\\'column2\\':\\'sum\\'})●\\nJoining\\nDataFrames\\n: df1.join(df2,df1[\\'id\\']==df2[\\'id\\'])●\\nSorting\\nData\\n: df.orderBy(df[\\'column\\'].desc())●\\nRemoving\\nDuplicates\\n: df.dropDuplicates()\\n4.HandlingMissingValues\\n●\\nDropping\\nRows\\nwith\\nMissing\\nValues\\n: df.na.drop()●\\nFilling\\nMissing\\nValues\\n: df.na.fill(value)●\\nReplacing\\nValues\\n: df.na.replace([\\'old_value\\'],[\\'new_value\\'])\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='5bfa90a0-bc32-48af-9abb-3929122e0bbe', embedding=None, metadata={'page_label': '2', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.DataTypeConversion\\n●\\nChanging\\nColumn\\nTypes\\n: df.withColumn(\\'column\\',df[\\'column\\'].cast(\\'new_type\\'))●\\nParsing\\nDates\\n: frompyspark.sql.functionsimportto_date;df.withColumn(\\'date\\',to_date(df[\\'date_string\\']))\\n6.AdvancedDataManipulations\\n●\\nUsing\\nSQL\\nQueries\\n: df.createOrReplaceTempView(\\'table\\');spark.sql(\\'SELECT*FROMtableWHEREcolumn> value\\')●\\nWindow\\nFunctions\\n: frompyspark.sql.windowimportWindow;frompyspark.sql.functionsimportrow_number;df.withColumn(\\'row\\',row_number().over(Window.partitionBy(\\'column\\').orderBy(\\'other_column\\')))●\\nPivot\\nTables\\n:df.groupBy(\\'column\\').pivot(\\'pivot_column\\').agg({\\'column2\\':\\'sum\\'})\\n7.DataLoading\\n●\\nWriting\\nto\\nCSV\\n: df.write.csv(\\'path/to/output\\')●\\nWriting\\nto\\nJSON\\n: df.write.json(\\'path/to/output\\')●\\nWriting\\nto\\nParquet\\n: df.write.parquet(\\'path/to/output\\')●\\nWriting\\nto\\na\\nDatabase\\n: df.write.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").save()\\n8.PerformanceTuning\\n●\\nCaching\\nData\\n: df.cache()●\\nBroadcasting\\na\\nDataFrame\\nfor\\nJoin\\nOptimization\\n: frompyspark.sql.functionsimportbroadcast;df1.join(broadcast(df2),df1[\\'id\\']==df2[\\'id\\'])●\\nRepartitioning\\nData\\n: df.repartition(10)●\\nCoalescing\\nPartitions\\n: df.coalesce(1)\\n9.DebuggingandErrorHandling\\n●\\nShowing\\nExecution\\nPlan\\n: df.explain() \\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='29ec1856-6de2-4519-8976-e74b1480ec38', embedding=None, metadata={'page_label': '3', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nCatching\\nExceptions\\nduring\\nRead\\n:\\nImplement\\ntry-except\\nblocks\\nduring\\ndata\\nreading\\noperations.\\n10.WorkingwithComplexDataTypes\\n●\\nExploding\\nArrays\\n: frompyspark.sql.functionsimportexplode;df.select(explode(df['array_column']))●\\nHandling\\nStruct\\nFields\\n: df.select('struct_column.field1','struct_column.field2')\\n11.CustomTransformationswithUDFs\\n●\\nDeﬁning\\na\\nUDF\\n: frompyspark.sql.functionsimportudf;@udf('return_type')defmy_udf(column):returntransformation●\\nApplying\\nUDF\\non\\nDataFrame\\n: df.withColumn('new_column',my_udf(df['column']))\\n12.WorkingwithLargeTextData\\n●\\nTokenizing\\nText\\nData\\n: frompyspark.ml.featureimportTokenizer;Tokenizer(inputCol='text_column',outputCol='words').transform(df)●\\nTF-IDF\\non\\nText\\nData\\n: frompyspark.ml.featureimportHashingTF,IDF;HashingTF(inputCol='words',outputCol='rawFeatures').transform(df)\\n13.MachineLearningIntegration\\n●\\nUsing\\nMLlib\\nfor\\nPredictive\\nModeling\\n:\\nBuilding\\nand\\ntraining\\nmachine\\nlearning\\nmodels\\nusing\\nPySpark's\\nMLlib.●\\nModel\\nEvaluation\\nand\\nTuning\\n: frompyspark.ml.evaluationimportMulticlassClassificationEvaluator;MulticlassClassificationEvaluator().evaluate(predictions)\\n14.StreamProcessing\\n●\\nReading\\nfrom\\na\\nStream\\n: dfStream=spark.readStream.format('source').load()●\\nWriting\\nto\\na\\nStream\\n: dfStream.writeStream.format('console').start()\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='3fc0f47b-56a8-4865-b432-ef00b76070bd', embedding=None, metadata={'page_label': '4', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"15.AdvancedDataExtraction\\n●\\nReading\\nfrom\\nMultiple\\nSources\\n: df=spark.read.format('format').option('option','value').load(['path1','path2'])●\\nIncremental\\nData\\nLoading\\n:\\nImplementing\\nlogic\\nto\\nload\\ndata\\nincrementally,\\nbased\\non\\ntimestamps\\nor\\nlog\\ntables.\\n16.ComplexDataTransformations\\n●\\nNested\\nJSON\\nParsing\\n: frompyspark.sql.functionsimportjson_tuple;df.select(json_tuple('json_column','field1','field2'))●\\nApplying\\nMap-Type\\nTransformations\\n:\\nUsing map\\nfunctions\\nto\\ntransform\\nkey-value\\npair\\ndata.\\n17.AdvancedJoinsandSetOperations\\n●\\nBroadcast\\nJoin\\nwith\\nLarge\\nand\\nSmall\\nDataFrames\\n:\\nUtilizingbroadcast\\nfor\\nefﬁcient\\njoins.●\\nSet\\nOperations\\n(Union,\\nIntersect,\\nExcept)\\n:\\nPerforming\\nset\\noperations\\nlike df1.union(df2)\\n, df1.intersect(df2)\\n,df1.except(df2)\\n.\\n18.DataAggregationandSummarization\\n●\\nComplex\\nAggregations\\n: df.groupBy('group_col').agg({'num_col1':'sum','num_col2':'avg'})●\\nRollup\\nand\\nCube\\nfor\\nMulti-Dimensional\\nAggregation\\n:df.rollup('col1','col2').sum()\\n, df.cube('col1','col2').mean()\\n19.AdvancedDataFiltering\\n●\\nFiltering\\nwith\\nComplex\\nConditions\\n: df.filter((df['col1']> value)&(df['col2']<other_value))●\\nUsing\\nColumn\\nExpressions\\n: frompyspark.sqlimportfunctionsasF;df.filter(F.col('col1').like('%pattern%'))\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='ff5d9526-435f-4b67-9021-53b8237569ae', embedding=None, metadata={'page_label': '5', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"20.WorkingwithDatesandTimes\\n●\\nDate\\nArithmetic\\n: df.withColumn('new_date',F.col('date_col')+F.expr('interval1 day'))●\\nDate\\nTruncation\\nand\\nFormatting\\n: df.withColumn('month',F.trunc('month','date_col'))\\n21.HandlingNestedandComplexStructures\\n●\\nWorking\\nwith\\nArrays\\nand\\nMaps\\n: df.select(F.explode('array_col'))\\n,df.select(F.col('map_col')['key'])●\\nFlattening\\nNested\\nStructures\\n: df.selectExpr('struct_col.*')\\n22.TextProcessingandNaturalLanguageProcessing\\n●\\nRegular\\nExpressions\\nfor\\nText\\nData\\n: df.withColumn('extracted',F.regexp_extract('text_col','(pattern)',1))●\\nSentiment\\nAnalysis\\non\\nText\\nData\\n:\\nUsing\\nNLP\\nlibraries\\nto\\nperform\\nsentiment\\nanalysis\\non\\ntextual\\ncolumns.\\n23.AdvancedWindowFunctions\\n●\\nWindow\\nFunctions\\nfor\\nRunning\\nTotals\\nand\\nMoving\\nAverages\\n: frompyspark.sql.windowimportWindow;windowSpec=Window.partitionBy('group_col').orderBy('date_col');df.withColumn('cumulative_sum',F.sum('num_col').over(windowSpec))●\\nRanking\\nand\\nRow\\nNumbering\\n: df.withColumn('rank',F.rank().over(windowSpec))\\n24.DataQualityandConsistencyChecks\\n●\\nData\\nProﬁling\\nfor\\nQuality\\nAssessment\\n:\\nGenerating\\nstatistics\\nfor\\neach\\ncolumn\\nto\\nassess\\ndata\\nquality.●\\nConsistency\\nChecks\\nAcross\\nDataFrames\\n:\\nComparing\\nschema\\nand\\nrow\\ncounts\\nbetween\\nDataFrames\\nfor\\nconsistency.\\n25.ETLPipelineMonitoringandLogging\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='a72fda39-61f0-48f8-8e51-1d6ba9afcfd9', embedding=None, metadata={'page_label': '6', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"●\\nImplementing\\nLogging\\nin\\nPySpark\\nJobs\\n:\\nUsing\\nPython's\\nlogging\\nmodule\\nto\\nlog\\nETL\\nprocess\\nsteps.●\\nMonitoring\\nPerformance\\nMetrics\\n:\\nTracking\\nexecution\\ntime\\nand\\nresource\\nutilization\\nof\\nETL\\njobs.\\n26.ETLWorkflowSchedulingandAutomation\\n●\\nIntegration\\nwith\\nWorkﬂow\\nManagement\\nTools\\n:\\nAutomating\\nPySpark\\nETL\\nscripts\\nusing\\ntools\\nlike\\nApache\\nAirﬂow\\nor\\nLuigi.●\\nScheduling\\nPeriodic\\nETL\\nJobs\\n:\\nSetting\\nup\\ncron\\njobs\\nor\\nusing\\nscheduler\\nservices\\nfor\\nregular\\nETL\\ntasks.\\n27.DataPartitioningandBucketing\\n●\\nPartitioning\\nData\\nfor\\nEfﬁcient\\nStorage\\n:df.write.partitionBy('date_col').parquet('path/to/output')●\\nBucketing\\nData\\nfor\\nOptimized\\nQuery\\nPerformance\\n:df.write.bucketBy(42,'key_col').sortBy('sort_col').saveAsTable('bucketed_table')\\n28.AdvancedSparkSQLTechniques\\n●\\nUsing\\nTemporary\\nViews\\nfor\\nSQL\\nQueries\\n:df.createOrReplaceTempView('temp_view');spark.sql('SELECT*FROMtemp_viewWHEREcol> value')●\\nComplex\\nSQL\\nQueries\\nfor\\nData\\nTransformation\\n:\\nUtilizing\\nadvanced\\nSQL\\nsyntax\\nfor\\ncomplex\\ndata\\ntransformations.\\n29.MachineLearningPipelines\\n●\\nCreating\\nand\\nTuning\\nML\\nPipelines\\n:\\nUsing\\nPySpark's\\nMLlib\\nfor\\nbuilding\\nand\\ntuning\\nmachine\\nlearning\\npipelines.●\\nFeature\\nEngineering\\nin\\nML\\nPipelines\\n:\\nImplementing\\nfeature\\ntransformers\\nand\\nselectors\\nwithin\\nML\\npipelines.\\n30.IntegrationwithOtherBigDataTools\\nBy:\\nWaleed\\nMousa\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}'),\n",
      " Document(id_='cd7a7ce4-814a-405a-9796-1edf083cdee2', embedding=None, metadata={'page_label': '7', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='●\\nReading\\nand\\nWriting\\nData\\nto\\nHDFS\\n:\\nAccessing\\nHadoop\\nDistributed\\nFile\\nSystem\\n(HDFS)\\nfor\\ndata\\nstorage\\nand\\nretrieval.●\\nInterfacing\\nwith\\nKafka\\nfor\\nReal-Time\\nData\\nProcessing\\n:\\nConnecting\\nto\\nApache\\nKafka\\nfor\\nstream\\nprocessing\\ntasks.\\n31.Cloud-SpecificPySparkOperations\\n●\\nUtilizing\\nCloud-Speciﬁc\\nStorage\\nOptions\\n:\\nLeveraging\\nAWS\\nS3,\\nAzure\\nBlob\\nStorage,\\nor\\nGCP\\nStorage\\nin\\nPySpark.●\\nCloud-Based\\nData\\nProcessing\\nServices\\nIntegration\\n:\\nUsing\\nservices\\nlike\\nAWS\\nGlue\\nor\\nAzure\\nSynapse\\nfor\\nETL\\nprocesses.\\n32.SecurityandComplianceinETL\\n●\\nImplementing\\nData\\nEncryption\\nand\\nSecurity\\n:\\nSecuring\\ndata\\nat\\nrest\\nand\\nin\\ntransit\\nduring\\nETL\\nprocesses.●\\nCompliance\\nwith\\nData\\nProtection\\nRegulations\\n:\\nAdhering\\nto\\nGDPR,\\nHIPAA,\\nor\\nother\\nregulations\\nin\\ndata\\nprocessing.\\n33.OptimizingETLProcessesforScalability\\n●\\nDynamic\\nResource\\nAllocation\\nfor\\nETL\\nJobs\\n:\\nAdjusting\\nSpark\\nconﬁgurations\\nfor\\noptimal\\nresource\\nusage.●\\nBest\\nPractices\\nfor\\nScaling\\nETL\\nProcesses\\n:\\nTechniques\\nfor\\nscaling\\nETL\\npipelines\\nto\\nhandle\\ngrowing\\ndata\\nvolumes.\\nBy:\\nWaleed\\nMousa\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "# Verify the documents after metadata configuration\n",
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca4745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Content for Embedding ===\n",
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View what the embedding model will see (without page_label and file_path)\n",
    "print(\"=== Content for Embedding ===\")\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17ffc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Content for LLM ===\n",
      "Metadata:\n",
      "page_label: 1\n",
      "file_path: c:\\Users\\Ibrahim\\Documents\\WORK\\Faculty-Projects\\nlp\\notebook\\..\\data\\Transformations in pyspark .pdf\n",
      "---\n",
      "Content:\n",
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n"
     ]
    }
   ],
   "source": [
    "# View what the LLM will see (includes all metadata)\n",
    "print(\"=== Content for LLM ===\")\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e0f4e",
   "metadata": {},
   "source": [
    "### Example: Creating a Custom Document with Metadata\n",
    "\n",
    "Demonstration of how to create a document from scratch with custom metadata configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bbe786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom document with specific metadata handling\n",
    "document = Document(\n",
    "    text=\"This framework is amazing!\",\n",
    "    metadata={\n",
    "        \"filename\": \"spark the definitive guide\",\n",
    "        \"category\": \"technology\",\n",
    "        \"author\": \"ibrahim\"\n",
    "    },\n",
    "    # Exclude 'filename' from both LLM and embedding views\n",
    "    excluded_llm_metadata_keys=[\"filename\"],\n",
    "    excluded_embed_metadata_keys=[\"filename\"],\n",
    "    # Customize how metadata is formatted\n",
    "    metadata_separator=\"\\n\",\n",
    "    metadata_template=\"{key}=>{value}\",\n",
    "    text_template=\"Metadata: \\n{metadata_str}\\n-------\\n Content=>{content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8775cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The LLM sees this ===\n",
      "Metadata: \n",
      "category=>technology\n",
      "author=>ibrahim\n",
      "-------\n",
      " Content=>This framework is amazing!\n"
     ]
    }
   ],
   "source": [
    "# View what the LLM sees for the custom document\n",
    "print(\"=== The LLM sees this ===\")\n",
    "print(document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e5a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The embedding model sees this ===\n",
      "Metadata: \n",
      "category=>technology\n",
      "author=>ibrahim\n",
      "-------\n",
      " Content=>This framework is amazing!\n"
     ]
    }
   ],
   "source": [
    "# View what the embedding model sees for the custom document\n",
    "print(\"=== The embedding model sees this ===\")\n",
    "print(document.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89a83d",
   "metadata": {},
   "source": [
    "## 4. LLM Setup and Document Transformation\n",
    "\n",
    "Set up the Groq LLM and create an ingestion pipeline that:\n",
    "- Splits documents into chunks\n",
    "- Extracts titles using LLM\n",
    "- Generates questions that each chunk can answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf8a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up Groq API key securely\n",
    "# os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a3032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Groq LLM for document transformations\n",
    "# # Using groq/compound-mini model for fast inference\n",
    "# llm_transformations = Groq(\n",
    "#     model=\"groq/compound-mini\", \n",
    "#     api_key=os.environ[\"GROQ_API_KEY\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d949cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an ingestion pipeline with multiple transformations\n",
    "# # 1. Text Splitter: Chunks documents into manageable pieces\n",
    "# text_splitter = SentenceSplitter(\n",
    "#     separator=\" \",           # Split on spaces\n",
    "#     chunk_size=20248,        # Max tokens per chunk\n",
    "#     chunk_overlap=128        # Overlap between chunks for context continuity\n",
    "# )\n",
    "\n",
    "# # 2. Title Extractor: Generates descriptive titles for chunks using LLM\n",
    "# title_extractor = TitleExtractor(\n",
    "#     llm=llm_transformations,\n",
    "#     nodes=3                  # Use 3 nodes for context when generating titles\n",
    "# )\n",
    "\n",
    "# # 3. Q&A Extractor: Generates questions that each chunk can answer\n",
    "# qa_extractor = QuestionsAnsweredExtractor(\n",
    "#     llm=llm_transformations,\n",
    "#     questions=3              # Generate 3 questions per chunk\n",
    "# )\n",
    "\n",
    "# # Combine all transformations into a pipeline\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=[\n",
    "#         text_splitter,\n",
    "#         title_extractor,\n",
    "#         qa_extractor\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Run the pipeline on the documents\n",
    "# # in_place=True modifies the original documents\n",
    "# nodes = pipeline.run(\n",
    "#     documents=docs,\n",
    "#     in_place=True,\n",
    "#     show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4f891",
   "metadata": {},
   "source": [
    "## 4.1. Alternative: Using Ollama (Deployed LLM Service)\n",
    "\n",
    "Instead of using Groq, you can use your deployed Ollama service for transformations and querying.\n",
    "Ollama is running at http://localhost:11434 in your Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2aa9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 01:09:35,775 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama Test Response ===\n",
      "I'm glad you're here. Unfortunately, I'm a large language model, I don't have the ability to hear or see you in the classical sense. I communicate with you through text-based interactions only. However, I can still chat with you and respond to your questions and statements! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "# Import Ollama integration\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize Ollama LLM pointing to your Docker service\n",
    "# Make sure your Ollama container is running and has a model pulled\n",
    "# Use the FULL model name with tag as shown in 'ollama list'\n",
    "# Note: Use smaller models if you have limited memory (see configuration tips below)\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    keep_alive=0,          # unload immediately after request\n",
    "    context_window=1024,   # smaller context\n",
    "    request_timeout=300.0,     # 5 minute timeout for slow responses\n",
    "    temperature=0.1 \n",
    "    )\n",
    "\n",
    "\n",
    "# Test the Ollama connection\n",
    "test_response = ollama_llm.complete(\"Hello! Can you hear me?\")\n",
    "\n",
    "print(\"=== Ollama Test Response ===\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda3965",
   "metadata": {},
   "source": [
    "### Check System Memory & Choose Model\n",
    "\n",
    "Before connecting to Ollama, check your available memory and choose an appropriate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c993e",
   "metadata": {},
   "source": [
    "### Using Ollama for Document Transformations\n",
    "\n",
    "Create an ingestion pipeline using Ollama instead of Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6636282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 7/7 [00:00<00:00, 1416.72it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]2025-12-28 01:24:58,834 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:02,194 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:04,753 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:06,106 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:09,000 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 14%|█▍        | 1/7 [00:16<01:38, 16.35s/it]2025-12-28 01:25:13,146 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 29%|██▊       | 2/7 [00:20<00:45,  9.17s/it]2025-12-28 01:25:17,151 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 3/7 [00:24<00:27,  6.81s/it]2025-12-28 01:25:19,855 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 4/7 [00:27<00:15,  5.19s/it]2025-12-28 01:25:22,733 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:24,997 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:27,557 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-28 01:25:30,646 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 71%|███████▏  | 5/7 [00:37<00:14,  7.21s/it]2025-12-28 01:25:32,927 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▌ | 6/7 [00:40<00:05,  5.53s/it]2025-12-28 01:25:34,969 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 7/7 [00:42<00:00,  6.05s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]2025-12-28 01:25:43,309 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 14%|█▍        | 1/7 [00:08<00:50,  8.34s/it]2025-12-28 01:25:48,098 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 29%|██▊       | 2/7 [00:13<00:31,  6.25s/it]2025-12-28 01:25:52,652 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 3/7 [00:17<00:21,  5.48s/it]2025-12-28 01:25:57,123 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 4/7 [00:22<00:15,  5.08s/it]2025-12-28 01:26:01,340 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 71%|███████▏  | 5/7 [00:26<00:09,  4.77s/it]2025-12-28 01:26:08,429 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▌ | 6/7 [00:33<00:05,  5.56s/it]2025-12-28 01:26:13,936 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 7/7 [00:38<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7 nodes with Ollama transformations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create transformations using Ollama LLM\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "text_splitter_ollama = SentenceSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=1024,         # Smaller chunks for faster processing\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Title extractor using Ollama\n",
    "title_extractor_ollama = TitleExtractor(\n",
    "    llm=ollama_llm,\n",
    "    nodes=2\n",
    ")\n",
    "\n",
    "# Q&A extractor using Ollama\n",
    "qa_extractor_ollama = QuestionsAnsweredExtractor(\n",
    "    llm=ollama_llm,\n",
    "    questions=3\n",
    ")\n",
    "\n",
    "# Create pipeline with Ollama-based extractors\n",
    "pipeline_ollama = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter_ollama,\n",
    "        title_extractor_ollama,\n",
    "        qa_extractor_ollama\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run the pipeline on documents\n",
    "# Note: This might take longer than Groq depending on your GPU\n",
    "async def run_pipeline():\n",
    "    return await pipeline_ollama.arun(documents=docs, in_place=False, show_progress=True)\n",
    "\n",
    "nodes_ollama = asyncio.get_event_loop().run_until_complete(run_pipeline())\n",
    "\n",
    "print(f\"Created {len(nodes_ollama)} nodes with Ollama transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106df81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe01f2f",
   "metadata": {},
   "source": [
    "### Interactive Chat with Ollama\n",
    "\n",
    "Create a simple chat interface to interact with your deployed Ollama service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565c383",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Simple chat function with Ollama\n",
    "def chat_with_ollama(message, llm=ollama_llm):\n",
    "    \"\"\"\n",
    "    Send a message to Ollama and get a response.\n",
    "    \n",
    "    Args:\n",
    "        message: User's question or prompt\n",
    "        llm: Ollama LLM instance\n",
    "    \n",
    "    Returns:\n",
    "        Response from the LLM\n",
    "    \"\"\"\n",
    "    response = llm.complete(message)\n",
    "    return response.text\n",
    "\n",
    "# Example interactions\n",
    "print(\"=== Chat Example 1 ===\")\n",
    "response1 = chat_with_ollama(\"What is machine learning in simple terms?\")\n",
    "print(response1)\n",
    "\n",
    "print(\"\\n=== Chat Example 2 ===\")\n",
    "response2 = chat_with_ollama(\"Explain the difference between supervised and unsupervised learning.\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e4b87",
   "metadata": {},
   "source": [
    "### RAG Query Engine with Ollama\n",
    "\n",
    "Use Ollama with your vector index for RAG-based question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9645359",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a vector index with Ollama-transformed nodes\n",
    "# Use the nodes created with Ollama or the original nodes\n",
    "index_ollama = VectorStoreIndex(nodes_ollama, embed_model=hf_embeddings)\n",
    "\n",
    "# Create query engine with Ollama LLM\n",
    "query_engine_ollama = index_ollama.as_query_engine(\n",
    "    llm=ollama_llm,\n",
    "    similarity_top_k=3  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "# Query the documents using Ollama\n",
    "user_question = \"What is this document about?\"\n",
    "print(f\"=== Question: {user_question} ===\\n\")\n",
    "\n",
    "response_ollama = query_engine_ollama.query(user_question)\n",
    "print(\"=== Ollama RAG Response ===\")\n",
    "print(response_ollama)\n",
    "\n",
    "# Show the source nodes used\n",
    "print(\"\\n=== Retrieved Source Chunks ===\")\n",
    "for i, node in enumerate(response_ollama.source_nodes, 1):\n",
    "    print(f\"\\n--- Source {i} (Score: {node.score:.4f}) ---\")\n",
    "    print(node.text[:300] + \"...\")  # Show first 300 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e10f7",
   "metadata": {},
   "source": [
    "### Interactive Q&A Loop with Ollama\n",
    "\n",
    "Create an interactive loop for continuous questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47730fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Interactive Q&A function\n",
    "def interactive_qa_with_ollama(query_engine, max_questions=5):\n",
    "    \"\"\"\n",
    "    Interactive Q&A session with Ollama RAG system.\n",
    "    \n",
    "    Args:\n",
    "        query_engine: The query engine to use\n",
    "        max_questions: Maximum number of questions (default: 5)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Interactive Q&A with Ollama (type 'exit' or 'quit' to stop)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    question_count = 0\n",
    "    \n",
    "    while question_count < max_questions:\n",
    "        # Get user input\n",
    "        user_query = input(f\"\\n[Question {question_count + 1}] You: \").strip()\n",
    "        \n",
    "        # Check for exit\n",
    "        if user_query.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"Exiting interactive session. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        # Query the engine\n",
    "        print(f\"\\n[Ollama]: Thinking...\", end=\"\")\n",
    "        try:\n",
    "            response = query_engine.query(user_query)\n",
    "            print(\"\\r\" + \" \" * 30 + \"\\r\", end=\"\")  # Clear \"Thinking...\"\n",
    "            print(f\"[Ollama]: {response}\\n\")\n",
    "            \n",
    "            # Optionally show sources\n",
    "            show_sources = input(\"Show source chunks? (y/n): \").strip().lower()\n",
    "            if show_sources == 'y':\n",
    "                print(\"\\n--- Source Chunks ---\")\n",
    "                for i, node in enumerate(response.source_nodes, 1):\n",
    "                    print(f\"\\nSource {i} (Relevance: {node.score:.4f}):\")\n",
    "                    print(node.text[:200] + \"...\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "        \n",
    "        question_count += 1\n",
    "    \n",
    "    print(f\"\\nSession ended. Answered {question_count} questions.\")\n",
    "\n",
    "# Run interactive session\n",
    "# Uncomment the line below to start the interactive session\n",
    "# interactive_qa_with_ollama(query_engine_ollama, max_questions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87852965",
   "metadata": {},
   "source": [
    "### Inspect Transformed Nodes\n",
    "\n",
    "View the nodes after transformation to see the extracted metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aeeace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Excerpt from document]\n",
      "document_title: **End‑to‑End PySpark ETL Workflow: Environment Setup, Data Extraction, Transformation, and Missing‑Value Handling**\n",
      "questions_this_excerpt_can_answer: 1. What exact code snippet does the document give for creating a SparkSession named **'ETLProcess'**?\n",
      "\n",
      "2. Which three `DataFrame.na` methods are listed for handling missing values, and what are the precise method calls shown for each (dropping rows, filling values, and replacing values)?\n",
      "\n",
      "3. According to the document’s metadata, what is the file size (in bytes) of **“Transformations in pyspark .pdf”**?\n",
      "Excerpt:\n",
      "-----\n",
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n",
      "-----\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# View what the embedding model sees for the first node\n",
    "print(\"=== Content for Embedding (First Node) ===\")\n",
    "print(nodes[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextNode(id_='d853ddbd-4435-4b3d-8904-b264063852b0', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13', 'document_title': '**End‑to‑End PySpark ETL Workflow: Environment Setup, Data Extraction, Transformation, and Missing‑Value Handling**', 'questions_this_excerpt_can_answer': \"1. What exact code snippet does the document give for creating a SparkSession named **'ETLProcess'**?\\n\\n2. Which three\\u202f`DataFrame.na`\\u202fmethods are listed for handling missing values, and what are the precise method calls shown for each (dropping rows, filling values, and replacing values)?\\n\\n3. According to the document’s metadata, what is the file size (in bytes) of **“Transformations in pyspark .pdf”**?\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label', 'file_path'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c8256f52-85f4-4ccd-800d-04cdbd75c3a9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'Transformations in pyspark .pdf', 'file_path': 'c:\\\\Users\\\\Ibrahim\\\\Documents\\\\WORK\\\\Faculty-Projects\\\\nlp\\\\notebook\\\\..\\\\data\\\\Transformations in pyspark .pdf', 'file_type': 'application/pdf', 'file_size': 101697, 'creation_date': '2025-12-27', 'last_modified_date': '2025-12-13'}, hash='a46ecdb35ad446fd76c46fd3796fe71d4c2da7a308cc1b89061ade4362a4e8f9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[ \\nE T L \\np r o c e s s e s \\nu s i n g \\nP y S p a r k \\n] \\n# \\nQ u i c k \\nS u m m a r y \\n 1.EnvironmentSetupandSparkSessionCreation\\n●\\nInstall\\nPySpark\\n: pipinstallpyspark●\\nStart\\na\\nSparkSession\\n: frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName(\\'ETLProcess\\').getOrCreate()\\n2.DataExtraction\\n●\\nRead\\nData\\nfrom\\nCSV\\n: df= spark.read.csv(\\'path/to/csv\\',inferSchema=True,header=True)●\\nRead\\nData\\nfrom\\nJSON\\n: df= spark.read.json(\\'path/to/json\\')●\\nRead\\nData\\nfrom\\nParquet\\n: df= spark.read.parquet(\\'path/to/parquet\\')●\\nRead\\nData\\nfrom\\na\\nDatabase\\n: df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\\n3.DataTransformation\\n●\\nSelecting\\nColumns\\n: df.select(\\'column1\\',\\'column2\\')●\\nFiltering\\nData\\n: df.filter(df[\\'column\\']> value)●\\nAdding\\nNew\\nColumns\\n: df.withColumn(\\'new_column\\',df[\\'column\\']+10)●\\nRenaming\\nColumns\\n: df.withColumnRenamed(\\'old_name\\',\\'new_name\\')●\\nGrouping\\nand\\nAggregating\\nData\\n: df.groupBy(\\'column\\').agg({\\'column2\\':\\'sum\\'})●\\nJoining\\nDataFrames\\n: df1.join(df2,df1[\\'id\\']==df2[\\'id\\'])●\\nSorting\\nData\\n: df.orderBy(df[\\'column\\'].desc())●\\nRemoving\\nDuplicates\\n: df.dropDuplicates()\\n4.HandlingMissingValues\\n●\\nDropping\\nRows\\nwith\\nMissing\\nValues\\n: df.na.drop()●\\nFilling\\nMissing\\nValues\\n: df.na.fill(value)●\\nReplacing\\nValues\\n: df.na.replace([\\'old_value\\'],[\\'new_value\\'])\\nBy:\\nWaleed\\nMousa', mimetype='text/plain', start_char_idx=0, end_char_idx=1370, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Inspect the full structure of the first node\n",
    "print(\"=== First Node Structure ===\")\n",
    "pprint.pprint(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5793a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Excerpt from document]\n",
      "page_label: 1\n",
      "file_path: c:\\Users\\Ibrahim\\Documents\\WORK\\Faculty-Projects\\nlp\\notebook\\..\\data\\Transformations in pyspark .pdf\n",
      "document_title: **End‑to‑End PySpark ETL Workflow: Environment Setup, Data Extraction, Transformation, and Missing‑Value Handling**\n",
      "questions_this_excerpt_can_answer: 1. What exact code snippet does the document give for creating a SparkSession named **'ETLProcess'**?\n",
      "\n",
      "2. Which three `DataFrame.na` methods are listed for handling missing values, and what are the precise method calls shown for each (dropping rows, filling values, and replacing values)?\n",
      "\n",
      "3. According to the document’s metadata, what is the file size (in bytes) of **“Transformations in pyspark .pdf”**?\n",
      "Excerpt:\n",
      "-----\n",
      "[ \n",
      "E T L \n",
      "p r o c e s s e s \n",
      "u s i n g \n",
      "P y S p a r k \n",
      "] \n",
      "# \n",
      "Q u i c k \n",
      "S u m m a r y \n",
      " 1.EnvironmentSetupandSparkSessionCreation\n",
      "●\n",
      "Install\n",
      "PySpark\n",
      ": pipinstallpyspark●\n",
      "Start\n",
      "a\n",
      "SparkSession\n",
      ": frompyspark.sqlimportSparkSession;spark=SparkSession.builder.appName('ETLProcess').getOrCreate()\n",
      "2.DataExtraction\n",
      "●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "CSV\n",
      ": df= spark.read.csv('path/to/csv',inferSchema=True,header=True)●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "JSON\n",
      ": df= spark.read.json('path/to/json')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "Parquet\n",
      ": df= spark.read.parquet('path/to/parquet')●\n",
      "Read\n",
      "Data\n",
      "from\n",
      "a\n",
      "Database\n",
      ": df=spark.read.format(\"jdbc\").option(\"url\",jdbc_url).option(\"dbtable\",\"table_name\").option(\"user\",\"username\").option(\"password\",\"password\").load()\n",
      "3.DataTransformation\n",
      "●\n",
      "Selecting\n",
      "Columns\n",
      ": df.select('column1','column2')●\n",
      "Filtering\n",
      "Data\n",
      ": df.filter(df['column']> value)●\n",
      "Adding\n",
      "New\n",
      "Columns\n",
      ": df.withColumn('new_column',df['column']+10)●\n",
      "Renaming\n",
      "Columns\n",
      ": df.withColumnRenamed('old_name','new_name')●\n",
      "Grouping\n",
      "and\n",
      "Aggregating\n",
      "Data\n",
      ": df.groupBy('column').agg({'column2':'sum'})●\n",
      "Joining\n",
      "DataFrames\n",
      ": df1.join(df2,df1['id']==df2['id'])●\n",
      "Sorting\n",
      "Data\n",
      ": df.orderBy(df['column'].desc())●\n",
      "Removing\n",
      "Duplicates\n",
      ": df.dropDuplicates()\n",
      "4.HandlingMissingValues\n",
      "●\n",
      "Dropping\n",
      "Rows\n",
      "with\n",
      "Missing\n",
      "Values\n",
      ": df.na.drop()●\n",
      "Filling\n",
      "Missing\n",
      "Values\n",
      ": df.na.fill(value)●\n",
      "Replacing\n",
      "Values\n",
      ": df.na.replace(['old_value'],['new_value'])\n",
      "By:\n",
      "Waleed\n",
      "Mousa\n",
      "-----\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# View what the LLM sees for the first node (includes extracted title and Q&A)\n",
    "print(\"=== Content for LLM (First Node) ===\")\n",
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa904971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Check total number of nodes created after chunking\n",
    "# print(f\"Total number of nodes created: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae679a9",
   "metadata": {},
   "source": [
    "## 5. Embedding Generation\n",
    "\n",
    "Create embeddings using HuggingFace's BGE model and build a vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a64627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 22:14:39,423 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "c:\\Users\\Ibrahim\\Documents\\WORK\\Faculty-Projects\\nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ibrahim\\AppData\\Local\\llama_index\\llama_index\\Cache\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-12-27 22:14:43,939 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-12-27 22:47:33,153 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02017112821340561, 0.14482395350933075, -0.01423242874443531, -0.07562268525362015, -0.009775497950613499, -0.025659609586000443, 0.08076140284538269, 0.012043717317283154, 0.017221175134181976, -0.012245615012943745, -0.018579687923192978, -0.050895098596811295, -0.0167841874063015, 0.011886931024491787, 0.05391477048397064, 0.02962633967399597, -0.004159002564847469, 0.00959134940057993, -0.08207977563142776, -0.04079286754131317, -0.007241450250148773, -0.020212257280945778, -0.02218056470155716, -0.020687056705355644, 0.02370595373213291, 0.010519594885408878, 0.011081664822995663, 0.0004888022085651755, -0.03809983655810356, -0.14868807792663574, 0.015195298008620739, 0.030558215454220772, 0.02603444643318653, 0.04514699801802635, -0.012815529480576515, 0.02152239717543125, -0.03646758198738098, 0.024992888793349266, -0.02396818809211254, -0.017713231965899467, 0.0617869608104229, 0.024671830236911774, -0.01000327430665493, -0.11049612611532211, -0.0161515474319458, -0.0323573499917984, 0.04589328169822693, 0.02165347710251808, 0.06752508133649826, 0.09011591970920563, -0.0515846386551857, 0.00455573620274663, -0.007575664669275284, 0.03516509383916855, -0.023194001987576485, 0.038747504353523254, 0.041843950748443604, 0.022240513935685158, 0.021269487217068672, 0.048500992357730865, 0.03882076218724251, 0.006993887014687061, -0.19470131397247314, -0.00660818163305521, -0.037991661578416824, -0.038424864411354065, 0.06329312175512314, -0.07798553258180618, 0.0008803823147900403, -0.01116653997451067, -0.03771156817674637, -0.0059681604616343975, 0.06394243985414505, 0.011137468740344048, 0.018614687025547028, -0.017004381865262985, 0.04847724735736847, -0.010910574346780777, -0.052246905863285065, 0.009804477915167809, 0.02003146894276142, -0.014493203721940517, -0.039901990443468094, -0.05469760671257973, 0.0008319886401295662, -0.050146039575338364, 0.007447008043527603, -0.020267833024263382, -0.006163787562400103, 0.07506114989519119, -0.030733546242117882, 0.02510739676654339, 0.005674019921571016, -0.06293027102947235, 0.0014490154571831226, -0.04732411727309227, -0.04267251119017601, -0.0633617639541626, -0.038817036896944046, 0.29048100113868713, 0.004893933422863483, 0.01626393385231495, 0.08065995573997498, -0.0474529005587101, 0.03831278905272484, 0.07038534432649612, 0.04130132868885994, -0.03731358051300049, -0.0009508938528597355, 0.03989014774560928, -0.07940743863582611, -0.01786184310913086, 0.0788973867893219, -0.07327945530414581, 0.03163480758666992, -0.030577152967453003, -0.0002261460031149909, -0.03086029179394245, -0.02922365441918373, 0.02993905358016491, -0.040982238948345184, 0.016839247196912766, -0.04519068822264671, 0.02178669162094593, -0.03429025039076805, -0.03678728640079498, -0.020777925848960876, 0.11479227244853973, 0.0010570604354143143, 0.03707073628902435, 0.06218381226062775, -0.04995644837617874, -0.024287983775138855, 0.03412652760744095, 0.039740193635225296, 0.01688685081899166, -0.07647500932216644, -0.018232353031635284, -0.022581785917282104, -0.05762803182005882, 0.0019320398569107056, -0.08875346928834915, -0.046894725412130356, -0.033774904906749725, 0.018726196140050888, 0.021228644996881485, -0.025193262845277786, 0.020748084411025047, 0.021487917751073837, -0.04055258631706238, 0.01945062354207039, 0.01277695968747139, -0.0006771663320250809, -0.018465055152773857, -0.007738145999610424, 0.006139901466667652, 0.015032566152513027, 0.020744115114212036, -0.057966332882642746, -0.01938413828611374, -0.010136623866856098, -0.019596347585320473, 0.016755634918808937, 0.13063260912895203, -0.02594810724258423, -0.05861177295446396, -0.006713012233376503, 0.002098725875839591, -0.001493194024078548, -0.0062937261536717415, 0.04435903578996658, -0.004560280591249466, -0.07602023333311081, 0.007413636427372694, 0.09631779789924622, 0.043566543608903885, -0.03872976452112198, 0.035440269857645035, -0.05316562578082085, 0.07991935312747955, 0.030866678804159164, 0.00014086630835663527, 0.03489973768591881, 0.03851579129695892, -0.03801792114973068, 0.02259029448032379, -0.05952218174934387, 0.015251723118126392, 0.02483959309756756, -0.016619578003883362, -0.006965757347643375, 0.03544829785823822, -0.04899632930755615, -0.0034183445386588573, -0.013909462839365005, 0.010621685534715652, -0.004012265708297491, -0.00855875015258789, -0.01645246148109436, -0.022036636248230934, 0.04491201043128967, 0.00661084521561861, -0.05416066572070122, 0.02863008715212345, 0.048644907772541046, 0.00015942551544867456, -0.05886777117848396, -0.0014349258271977305, -0.03228636458516121, 0.0007802812615409493, 0.09230342507362366, 0.010104640386998653, 0.016940724104642868, 0.05638817697763443, -0.01659802719950676, -0.03142998367547989, -0.07813552767038345, 0.08060967922210693, -0.09343383461236954, -0.00047420471673831344, 0.0343874990940094, 0.008465634658932686, -0.027553103864192963, -0.23595231771469116, 0.06126578897237778, 0.021479494869709015, -0.008167732506990433, 0.008060874417424202, -0.017138326540589333, 0.04889494553208351, -0.03918730095028877, 0.05751512199640274, 0.04666073992848396, 0.032258231192827225, 0.061248816549777985, 0.014063484966754913, 0.10880526155233383, -0.11240990459918976, 0.04743904247879982, -0.0186023972928524, -0.009981812909245491, 0.011770453304052353, 0.02540639229118824, -0.02573413960635662, 0.04555748775601387, 0.02566068060696125, 0.03608275577425957, -0.0073571461252868176, -0.0474788062274456, 0.09196525812149048, 0.12223194539546967, 0.03243369981646538, -0.03168303519487381, 0.007246454246342182, 0.06550517678260803, -0.04741731658577919, -0.1397653967142105, 0.08874579519033432, 0.0867813378572464, 0.0046250903978943825, -0.04329003021121025, 0.012406996451318264, 0.0030864118598401546, 0.07962343841791153, 0.03921616077423096, 0.03218969702720642, -0.06094610318541527, -0.008936037309467793, -0.02761046588420868, 0.11103551089763641, -0.04390167072415352, 0.03490882366895676, -0.009532343596220016, 0.0013508857227861881, -0.020592160522937775, 0.03587668761610985, 0.017152724787592888, 0.01791299693286419, 0.05678556486964226, -0.07459433376789093, -0.07128060609102249, -0.020907806232571602, -0.005387481767684221, -0.006781551521271467, -0.04035819321870804, 0.021272849291563034, -0.0034131789579987526, -0.021091708913445473, -0.01639557257294655, -0.06794319301843643, -0.025850418955087662, 0.014530767686665058, -0.011721238493919373, -0.051848553121089935, -0.02689720317721367, -0.005456685088574886, -0.07641512155532837, 0.007313137874007225, -0.04719228297472, 0.034670937806367874, -0.07806906849145889, -0.009545637294650078, 0.03680157661437988, 0.07664546370506287, 0.003602720098569989, -0.004523403011262417, -0.0012036943808197975, 0.08199947327375412, 0.06294986605644226, 0.062027640640735626, 0.014433133415877819, 0.017925376072525978, 0.01832648180425167, 0.024761982262134552, -0.041879259049892426, -0.03179982677102089, 0.0317503847181797, 0.03836687281727791, 0.01929701305925846, -0.24154961109161377, 0.009592204354703426, -0.01585453562438488, 0.07304834574460983, -0.0005017469520680606, -0.028569631278514862, 0.023357605561614037, 0.04139501601457596, -0.03628646209836006, -0.045066893100738525, 0.04735486954450607, 0.004926488734781742, -0.013142789714038372, 0.04182756319642067, 0.024324685335159302, 0.0015832470962777734, 0.025879818946123123, 0.032493527978658676, -0.007519818842411041, 0.0019557191990315914, -0.05458879843354225, 0.0232557263225317, 0.12147729843854904, -0.06788887083530426, -0.029938071966171265, 0.08253137767314911, -0.060510434210300446, 0.000533656741026789, -0.013902340084314346, 0.03738950192928314, 0.13270039856433868, -0.023294320330023766, -0.031591299921274185, 0.018900634720921516, -0.0606377087533474, 0.10787466913461685, -0.03350348398089409, -0.008449355140328407, 0.011908316984772682, -0.039663203060626984, -0.025479000061750412, 0.04307180643081665, -0.02896365150809288, -0.01877722702920437, 0.034871265292167664, 0.0019553217571228743, -0.06407491862773895, 0.015249049291014671, -0.015002231113612652, -0.03305649384856224, 0.026144258677959442, -0.05880415067076683, -0.003810594556853175, 0.015299667604267597, 0.027435118332505226, 0.003657043445855379, -0.09397092461585999, -0.08305110037326813, -0.04832703620195389, 0.0025662241969257593, -0.033950887620449066, 0.020002461969852448, -0.005184954963624477, 0.061977896839380264, 0.02074887789785862]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embedding model\n",
    "# Using BAAI/bge-small-en-v1.5 - a small but effective embedding model\n",
    "hf_embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Test the embedding model with a sample text\n",
    "test_embed = hf_embeddings.get_text_embedding(\"Allez si ibrahim\")\n",
    "print(f\"Embedding dimension: {len(test_embed)}\")\n",
    "print(f\"Sample embedding values: {test_embed[:5]}\")  # Show first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081bfde",
   "metadata": {},
   "source": [
    "## 6. Vector Store and Indexing\n",
    "\n",
    "Create a vector index from the nodes for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c77417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store index from the nodes\n",
    "# This builds the index in-memory for fast querying\n",
    "index = VectorStoreIndex(nodes, embed_model=hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ee190",
   "metadata": {},
   "source": [
    "## 7. Query Engine\n",
    "\n",
    "Set up a query engine to perform RAG (Retrieval-Augmented Generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for querying (specify the appropriate Groq model)\n",
    "# Replace \"llamamodelname\" with an actual model like \"llama-3.3-70b-versatile\"\n",
    "llm_querying = Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # Or another available Groq model\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Create a query engine from the index\n",
    "query_engine = index.as_query_engine(llm=llm_querying)\n",
    "\n",
    "# Perform a sample query\n",
    "response = query_engine.query(\"What is this document about\")\n",
    "\n",
    "print(\"=== Query Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4243f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the source nodes used to generate the response\n",
    "# This shows which document chunks were retrieved and used\n",
    "print(\"=== Source Nodes ===\")\n",
    "pprint.pprint(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d207d",
   "metadata": {},
   "source": [
    "## 8. Persistent Storage with ChromaDB\n",
    "\n",
    "Store the vector index in ChromaDB for persistence across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc903764",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB for persistent storage\n",
    "# PersistentClient saves data to disk\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Get or create a collection for storing vectors\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# Create a ChromaDB vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create storage context with ChromaDB as the vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build the index with persistent storage\n",
    "# This will save embeddings to ChromaDB\n",
    "index = VectorStoreIndex(\n",
    "    nodes, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=hf_embeddings\n",
    ")\n",
    "\n",
    "# Alternative: Build index directly from documents with transformations\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, \n",
    "#     storage_context=storage_context, \n",
    "#     transformations=[text_splitter, title_extractor, qa_extractor]\n",
    "# )\n",
    "\n",
    "# Create query engine from the persistent index\n",
    "query_engine = index.as_query_engine(llm=llm_querying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240b7c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Query the persistent index\n",
    "response = query_engine.query(\"What is this document about\")\n",
    "\n",
    "print(\"=== Query Response (from ChromaDB) ===\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
